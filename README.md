âœˆï¸ Airline Data Ingestion â€“ Incremental ETL Pipeline
An Industrial-Grade Incremental Data Pipeline using Python, AWS Glue, Step Functions & Redshift
ğŸ“Œ Project Overview

The Airline Data Ingestion Pipeline is designed to ingest airport codes and daily flight data incrementally into an AWS-based data lake and warehouse environment.
This pipeline automates ingestion, processing, cataloging, and loading of airline datasets using AWS serverless components.

The project ensures:

Incremental processing using Job Bookmarking

Automated orchestration using AWS Step Functions

Catalog-driven metadata with Glue Crawler + Glue Catalog

Reliable notifications using SNS

Scalable storage + warehouse integration with S3 & Redshift

ğŸ—ï¸ Architecture Diagram
S3 (raw & daily flight data)
        |
        v
Glue Crawler ----> Glue Catalog ----> Redshift (preloaded + target tables)
        |
        v
Glue ETL Job (Incremental using Job Bookmark)
        |
        v
Step Functions Orchestration
        |
        v
EventBridge (Triggers SFN)
        |
        v
SNS Notifications (Success/Failure)

ğŸ”§ Tech Stack

Python

AWS S3

AWS Glue (Crawler, Catalog, ETL, Job Bookmarking)

AWS Step Functions

AWS EventBridge

AWS Redshift

AWS SNS

ğŸ“‚ Project Folder Structure
Airline-Data-Ingestion/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ airport_codes/        # Static reference dataset
â”‚   â”œâ”€â”€ flights_daily/        # Daily incremental files
â”‚
â”œâ”€â”€ glue-scripts/
â”‚   â”œâ”€â”€ incremental_flights_etl.py
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_airport_codes_table.sql
â”‚   â”œâ”€â”€ create_flights_target_table.sql
â”‚
â””â”€â”€ README.md

ğŸ› ï¸ Project Workflow
1ï¸âƒ£ Load airport codes & daily flight data into S3

Upload airport_codes.csv (static)

Upload daily flight files into the flights folder

2ï¸âƒ£ Create Redshift Tables

Preload airport_codes table

Create flights_target table structure

3ï¸âƒ£ Register Tables Using Glue Crawler

Crawler scans S3

Crawler scans Redshift

Tables added/updated in Glue Data Catalog

4ï¸âƒ£ Incremental Processing with Glue ETL

Glue ETL job reads new flight files only

Uses Job Bookmarking to avoid duplicates

Cleans & transforms data

Loads into Redshift flights_target table

5ï¸âƒ£ Pipeline Orchestration with Step Functions

Manages:

Starting Glue Job

Checking job status

Handling success/failure paths

6ï¸âƒ£ EventBridge Trigger

EventBridge triggers the Step Function based on:

Scheduled time (daily)

OR new file arrival in S3

7ï¸âƒ£ SNS Notifications

Sends email alerts on success

Sends alerts on failure with error details

ğŸ“Š Key Features

âœ” Incremental daily ingestion
âœ” Fully automated orchestration
âœ” Glue Crawler for schema management
âœ” Glue Job Bookmarking prevents duplicate loads
âœ” Redshift warehouse integration
âœ” Notifications for monitoring
âœ” Serverless, scalable, cost-efficient pipeline

â–¶ï¸ How to Run

Upload datasets to S3

Run Glue Crawler to register tables

Deploy Glue ETL job with Job Bookmark enabled

Create Step Functions workflow

Connect Step Function to EventBridge trigger

Subscribe email to SNS topic

Run the pipeline

ğŸ“¬ Contact

If you want me to generate:
âœ” Architecture diagram (image)
âœ” Glue ETL Python script (full code)
âœ” Step Functions definition (ASL JSON)
âœ” SQL scripts for Redshift

Just tell me â€” I can add them to the repo!
